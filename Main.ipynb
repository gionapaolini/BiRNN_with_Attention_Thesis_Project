{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ProposedModel as proposedModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [] #Load here the question/sentences to be classified\n",
    "articleIds = [] #Load here the outputs (labels/kbaID)\n",
    "\n",
    "outputs =  []#the text answer from the QA-System, only needed for Word2Vec training\n",
    "\n",
    "\n",
    "useAttention = True\n",
    "useBiRNN = True\n",
    "usePretrainedEmbeddings = True\n",
    "useDynamicEmbeddings = True\n",
    "\n",
    "\n",
    "batchSize = 5000\n",
    "epochs = 30\n",
    "dropout = 0.4\n",
    "rnn_layers = 2\n",
    "n_hidden_nodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "wordsPerInteractionOutput = []\n",
    "wordsPerInteractionInput = []\n",
    "\n",
    "\n",
    "print(\"Preprocessing output corpus..\")\n",
    "start = time.time()\n",
    "for index, text in enumerate(outputs):\n",
    "    processedWords = utils.preprocess(text).split(\" \")\n",
    "    words.extend(processedWords)\n",
    "    wordsPerInteractionOutput.append(processedWords)\n",
    "end = time.time()\n",
    "print(\"Done! Time passed {:.4f} sec\".format((end-start)))\n",
    "\n",
    "print(\"Preprocessing input corpus..\")\n",
    "start = time.time()\n",
    "for index, text in enumerate(inputs):\n",
    "    processedWords = utils.preprocess(text).split(\" \")\n",
    "    words.extend(processedWords)\n",
    "    wordsPerInteractionInput.append(processedWords)\n",
    "end = time.time()\n",
    "print(\"Done! Time passed {:.4f} sec\".format((end-start)))\n",
    "\n",
    "word_counts = Counter(words)\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load relevant GloVe embeddings \n",
    "Discard the one that do not appear in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(usePretrainedEmbeddings):\n",
    "    \n",
    "    setWords = set(words)    \n",
    "    \n",
    "    #Load GLOVE vectors\n",
    "    filepath_glove = 'glove.6B.300d.txt'\n",
    "    glove_vocab = []\n",
    "    glove_embd=[]\n",
    "    embedding_dict = {}\n",
    "\n",
    "    file = open(filepath_glove,'r',encoding='UTF-8')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab_word = row[0]\n",
    "        if vocab_word in setWords:\n",
    "            glove_vocab.append(vocab_word)\n",
    "            embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "            glove_embd.append(embed_vector)\n",
    "            embedding_dict[vocab_word]=embed_vector\n",
    "    file.close()\n",
    "\n",
    "    print('Loaded GLOVE')\n",
    "    \n",
    "    #Generate random vectors for words that were not found in GloVe\n",
    "    \n",
    "    for word in setWords:\n",
    "        if word not in glove_vocab:\n",
    "            glove_vocab.append(word)\n",
    "            embedding = 2 * np.random.random_sample(300) - 1\n",
    "            glove_embd.append(embedding)\n",
    "            embedding_dict[word]=embedding\n",
    "            \n",
    "            \n",
    "    #The first vector is a 0 vector for padding\n",
    "    glove_embd.insert(0, np.zeros(300))\n",
    "\n",
    "    #Cast into array\n",
    "    embeddings = np.asarray(glove_embd,dtype=np.float32)\n",
    "    vocabulary = glove_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or train embeddings on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(n_sentences_in_batch, sentences, window_size=5):  \n",
    "\n",
    "    for idx in range(0, len(sentences), n_sentences_in_batch):\n",
    "        x, y = [], []\n",
    "        batch = sentences[idx:idx+n_sentences_in_batch]\n",
    "        for batchIndex in range(len(batch)):\n",
    "            for wordIndex in range(len(batch[batchIndex])):\n",
    "                batch_x = batch[batchIndex][wordIndex]\n",
    "                batch_y = get_target(batch[batchIndex], wordIndex, window_size)\n",
    "                y.extend(batch_y)\n",
    "                x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not usePretrainedEmbeddings):\n",
    "    word2vecEpochs = 10\n",
    "    n_embedding = 300\n",
    "    n_sampled = 10\n",
    "    learning_rate = 0.001\n",
    "    window_size = 5\n",
    "    n_sentences_in_batch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not usePretrainedEmbeddings):\n",
    "    \n",
    "    for index, _ in enumerate(wordsPerInteractionInput):\n",
    "        wordsPerInteractionInput[index] = [word for word in wordsPerInteractionInput[index] if (word!=\"\")]\n",
    "    \n",
    "    for index, _ in enumerate(wordsPerInteractionOutput):\n",
    "        wordsPerInteractionOutput[index] = [word for word in wordsPerInteractionOutput[index] if (word!=\"\")]\n",
    "        \n",
    "    wordsPerInteraction = list(wordsPerInteractionOutput)\n",
    "    wordsPerInteraction.extend(wordsPerInteractionInput)\n",
    "    \n",
    "    int_wordsPerInteraction = []\n",
    "    \n",
    "    vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "\n",
    "    for index, interaction in enumerate(wordsPerInteraction):\n",
    "        int_wordsPerInteraction.append([vocab_to_int[word] for word in wordsPerInteraction[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not usePretrainedEmbeddings):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    n_vocab = len(int_to_vocab) + 1\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1), name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab),name='softmax_b')\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, word2vecEpochs+1):\n",
    "        sentences = get_batches(n_sentences_in_batch ,int_wordsPerInteraction, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in sentences:\n",
    "\n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "\n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, word2vecEpochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            iteration += 1\n",
    "            break\n",
    "        break\n",
    "        save_path = saver.save(sess, \"Word2VecCheckpoints/Epoch{}.ckpt\".format(e))\n",
    "        \n",
    "    embeddings = sess.run(normalized_embedding)\n",
    "    embeddings[0] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(usePretrainedEmbeddings):\n",
    "    vocab_to_int, int_to_vocab = utils.create_lookup_tables(vocabulary)\n",
    "    \n",
    "article_to_int, int_to_article = utils.create_lookup_tables(articleIds)\n",
    "\n",
    "\n",
    "int_article = [article_to_int[articleId] for articleId in articleIds]\n",
    "\n",
    "int_wordsPerInteractionInput = []\n",
    "\n",
    "for index, interaction in enumerate(wordsPerInteractionInput):\n",
    "    int_wordsPerInteractionInput.append([vocab_to_int[word] for word in wordsPerInteractionInput[index] if (word!=\"\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "if(useDynamicEmbeddings):\n",
    "    embeddings = tf.Variable(embeddings)\n",
    "    \n",
    "embed = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "out_dim = len(set(articleIds))\n",
    "onehot_encoding_articles = tf.one_hot(int_article, out_dim)\n",
    "labels = tf.nn.embedding_lookup(onehot_encoding_articles, y)\n",
    "\n",
    "maxLengthBatch = tf.placeholder(tf.int32)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "model = proposedModel.VariableSequenceClassification(embed,labels,maxLengthBatch,keep_prob,\n",
    "                                                     useAttention,useBiRNN, num_hidden = rnn_layers, num_layers = rnn_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset in Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(int_wordsPerInteractionInput, int_article, test_size=0.05, random_state = 24403)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('Checkpoints'))\n",
    "\n",
    "batches = utils.get_batches_rnn(batchSize, X_test, y_test)\n",
    "x_batchTest, y_batchTest, maxLengthTest = next(batches)    \n",
    "\n",
    "trainErr = []\n",
    "evalErr = []\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    batches = utils.get_batches_rnn(batchSize, X_train, y_train)\n",
    "    batchesCount = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for x_batch, y_batch, maxLength in batches:\n",
    "        _ , err = sess.run([model.optimize, model.error], {x: x_batch, y: y_batch, maxLengthBatch: maxLength, keep_prob: (1-dropout)})\n",
    "        if batchesCount % 10 == 0:\n",
    "            print('Epoch {:2d}, Step {:4d} error {:3.1f}%'.format(epoch,batchesCount, 100 * err))\n",
    "        batchesCount += 1\n",
    "        lastErr = err\n",
    "        \n",
    "    trainErr.append(lastErr)\n",
    "    \n",
    "    saver.save(sess, \"Checkpoints/epoch{}.ckpt\".format(epoch))\n",
    "    \n",
    "    error = sess.run(model.error, {x: x_batchTest, y: y_batchTest, maxLengthBatch: maxLengthTest, keep_prob: 1})\n",
    "    evalErr.append(error)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"---Eval---\")\n",
    "    print('Eval Error {:3.1f}%'.format(100 * error))\n",
    "    print(\"Epoch Time passed {:.4f} sec\".format((end-start)))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
